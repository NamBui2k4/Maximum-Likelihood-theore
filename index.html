---
layout: default
---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Maximum likelihood theory</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
</head>
<body>
    <p>Bài viết này được kham khảo từ nguồn:</p>
<ul>
<li><p><a href="https://phamdinhkhanh.github.io/deepai-book/ch_ml/NaiveBayes.html"><em>phamdinhkhanh.github</em></a></p>
</li>
<li><p><a href="https://kierandg.blogspot.com/2016/05/so-luoc-log-likelihood-maximum-likelihood-estimation.html"><em>kierandg.blogspot.com</em></a></p>
</li>
<li><p><a href="https://dangnguyenit.blogspot.com/2018/10/uoc-luong-hop-ly-cuc-aimaximum.html"><em>dangnguyenit.blogspot.com</em></a></p>
</li>
</ul>
<p><em>Lưu ý</em>: Bài viết này rát nhiều toán  (và sai chính tả nửa :v). Nếu không thích toán, bạn có thể bỏ qua.</p>
<p><em>Ghi chú: Tôi viết bài này để review lại những kiến thức xác suất thống kê mà bản thân đã học từ thời sinh viên của mình nên bài viết này không thực sự phục vụ những bạn đọc nào quá mới mẻ, đặc biệt là những bạn nào chưa thực sự vững vàng với toán học.</em></p>
<h1>Ước lượng khả năng tối đa (Maximum Likelihood Estimation - MLE)</h1>
<p>Trong thống kê và nghiên cứu khoa học nói chúng, dữ liệu thường được diễn tả thông qua những <strong>phân phối xác suất</strong>. Các phân phối xác suất thường được đặc trưng bởi những tham số nhất định. Ví dụ, đối với phân phối chuẩn, tham số đặc trưng chính là trung bình (<span class="math">\(μ\)</span>) và phương sai (<span class="math">\(σ^2\)</span>). Đối với phân phối Poisson
thì tham số đặc trưng là tỷ lệ trung bình thời gian <span class="math">\(λ\)</span>.</p>
<p>Tuy nhiên, có những tình huống mà chúng ta không thể đưa ra những con số tuyệt đối này khi thống kê trên một tổng thể lớn. Chẳng hạn, bạn muốn
thống kê chiều cao trung bình của người Việt Nam nhưng lại không thể đi khắp cái nước Việt Nam với 100 triệu người này và hỏi &quot;bạn cao bao nhiêu m vậy&quot;.
Bạn chỉ có thể đi hỏi khoảng 50 người và biết rằng chiều cao trung bình giữa họ là 1m62, nhưng lại không chắc chắn con số này có đúng với 100 triệu người không.
Đó là lúc mà giải pháp <strong>ước lượng khả năng tối đa (Maximum Likelihood Estimation - viết tắt là MLE) ra đời. Tuy nhiên, trước hết, ta cần hiểu rõ <strong>hàm khả năng (Likelihood function) là gì.</p>

<h2>Định nghĩa hàm khả năng</h2>
<pre><code>Trong thống kê, Likelihood function - gọi là
hàm khả năng, được hiểu là một hàm đo lường
mức độ &quot;phù hợp&quot; của một bộ tham số cụ thể với
dữ liệu quan sát được. 
</code></pre>
<p><br />
Giả sử biến ngẫu nhiên  X  tuân theo một phân phối nào đó được mô tả bởi bộ tham số <span class="math">\(θ ( θ_1,θ_2,...,θ_k )\)</span> mà ta chưa biết.</p>
<p>Hàm khả năng của phân phối có dạng:</p>
<ul>
<li><span class="math">\(L(θ)= f(x_1,x_2,...,x_n | θ_1,θ_2,...,θ_k)\)</span> (nếu  X  là biến liên tục)</li>
<li><span class="math">\(L(θ)= p(x_1,x_2,...,x_n | θ_1,θ_2,...,θ_k)\)</span> (nếu  X  là biến rời rạc)</li>
</ul>
<p>Trong đó:</p>
<ul>
<li><span class="math">\(x_1,x_2,...,x_n\)</span> là các giá trị mà X có thể nhận trong mẫu dữ liệu</li>
<li><span class="math">\(θ_1,θ_2,...,θ_k\)</span> là tập hợp tham số của phân phối dữ liệu</li>
</ul>
<p>Hàm khả năng có thể được hiểu là xác suất để các sự kiện  <span class="math">\(x_1,x_2,...,x_n\)</span> cùng xảy ra, với điều kiện <span class="math">\(( θ_1,θ_2,...,θ_k )\)</span>.
Như vậy, cách gọi &quot;hàm khả năng&quot; ở đây chính là xác suất có điều kiện.</p>
<hr />
<h2 id="inh-nghia-uoc-luong-kha-nang-toi-a-maximum-likelihood-definition">Định nghĩa ước lượng khả năng tối đa (Maximum Likelihood definition)</h2>
<pre><code>Trong thống kê, ước lượng khả năng tối đa là một
phương pháp ước lượng tham số của phân phối dữ
liệu bằng cách tối đa hoá hàm khả năng sao cho
dưới giả định của thống kê thì dữ liệu
trở nên phù hợp nhất.
</code></pre>
<p>Nói đơn giản, ước lượng khả năng tối đa là tìm θ để <span class="math">\(L(θ)\)</span> đạt max.</p>
<p><span class="math">\(θ = argmax{L(θ)}\)</span></p>
<p>Tưởng đơn giản ấy, nhưng mà không nhé. Vì ta phải giải quyết bài toán &quot;tính xác suất để X nhận được tất cả các giá trị <span class="math">\(x_1,x_2,...,x_n\)</span>&quot;. Mặt khác, ta đã nói rằng  hàm likelihood bản chất là xác suất có điều kiện. Khi đó, nếu <span class="math">\(X\)</span> liên tục, ta biến đổi như sau:</p>
<p><span class="math">\(L(θ) = f(x_1,x_2,...,x_n | θ) = f(x_1|θ).f(x_2|θ)...f(x_n|θ)\)</span></p>
<p>Nếu X rời rạc, ta biến đổi:</p>
<p><span class="math">\(L(θ)= p(x_1,x_2,...,x_n | θ_1,θ_2,...,θ_k) = p(x_1|θ) + p(x_2|θ) +...+ p(x_n|θ_k)\)</span></p>
<p>(Nói chung thì cũng chỉ là khác nhau giữa dấu nhân và dấu cộng :v )</p>
<p>Việc tính toán likelihood sẽ gặp rất nhiều trở ngại. Đấy là chưa kể ta không biết θ sẽ chứa cái gì (tùy thuộc vào loại phân phối mà ta giả định thì
θ có thể chứa μ, σ, λ,...). Lúc này bài toán có 2 kiểu: Ước lượng một <span class="math">\(θ\)</span> và Ước lượng nhiều <span class="math">\(θ\)</span>.</p>
<h2 id="uoc-luong-mot">Ước lượng một <span class="math">\(θ\)</span></h2>
<p>Giả định rằng ta biết trước phân phối dữ liệu (tức là biết trước bộ tham số <span class="math">\(θ\)</span> có cái gì) , bài toán lúc này yêu cầu chúng ta ước lượng likelihood cho một tham số trong bộ θ.</p>
<p>Có 5 bước để giải:</p>
<ol>
<li><p><strong>Xác định likelihood</strong></p>
</li>
<li><p><strong>Biến đổi likelihood về dạng logarit</strong>: <span class="math">\(u(θ) = log(L(θ))\)</span>.</p>
</li>
<li><p><strong>Đạo hàm riêng</strong>: <span class="math">\(u(θ)' = log(L(θ))'\)</span></p>
</li>
<li><p><strong>likelihood đạt tối đa</strong></p>
<p>⇔ \(u(θ)\) đạt tối đa</p>
<p>⟺ <span class="math">\(u(θ)' = 0\)</span></p>
<p>⟺ tham số trong <span class="math">\(θ\)</span> bằng một giá trị nào đó</p>
</li>
<li><p><strong>kết luận</strong> <span class="math">\(θ\)</span></p>
</li>
</ol>
<hr />
<p><strong>Ví dụ: Cho  \(x_1,x_2,...,x_n\)∼  Bernoulli(p). Dùng phương pháp MLE để ước lượng tham số <span class="math">\(p\)</span> .</p>
<p><em>Giải:</em></p>
<p><span class="math">\(Bernoulli(p)\)</span> là phân phối xác suất của biến liên tục x. Hàm phân phối xác suất của nó là: <span class="math">\(\quad f(x|θ) = p^{x}(1−p)^{1−x}\)</span></p>
<ol>
<li>Xác định Likelihood:
<span class="math">\(L(p) = f(x_1,x_2,...,x_n | p) = f(x_1|p).f(x_2|p)...f(x_n|p)\)</span></li>
</ol>
<p><span class="math">\(=∏_{i = 1}^{n} p^{x_i}(1−p)^{1−x_i}\)</span></p>
<li>Dùng logarit đưa likelihood về dạng tổng</li>
<p>Đặt \( u(p) = log(L(p)) \), ta có:</p>
<p><span class="math">\(u(p) = log(∏_{i = 1}^{n} p^{x_i}(1−p)^{1−x_i} )\)</span></p>
<p><span class="math">\(= ∑_{i = 1}^{n} log(p^{x_i}(1−p)^{1−x_i})\)</span></p>
<p><span class="math">\(= ∑_{i=1}^{n} log(p^{x_i})+ ∑_{i=1}^{n}log(1−p)^{1−x_i}\)</span></p>
<p><span class="math">\(= ∑_{i=1}^{n} x_i \ log(p) + ∑_{i=1}^{n}(1−x_i) \ log(1−p)\)</span></p>
<p>Đặt  <span class="math">\(∑_{i=1}^{n} x_i = t\)</span>, ta coi t như một hằng số thì phương trình trở thành:</p>
<p><span class="math">\(u(p) = t  \  log(p) + (n - t)  \  log(1-p)\)</span></p>
<li><p>đạo hàm: <span class="math">\(u'(p) = t \ \frac{1}{p} +(n−t) \  \frac{−1}{1−p}\)</span></p>
</li>
<li><p>likelihood đạt tối đa</p>
</li>
<p>⟺ <span class="math">\(u(p)\)</span> đạt tối đa</p>
<p>⟺ \(u'(p) = 0 \)</p>
<p>⟺ <span class="math">\(t \ \frac{1}{p} +(n−t) \  \frac{−1}{1−p}=0\)</span></p>
<p>⟺ <span class="math">\(p = \frac{1}{n} ∑_{i=1}^{n}x_i\)</span></p>
<ol start="6">
<li>Kết luận:  <span class="math">\(p = \frac{1}{n} ∑_{i=1}^{n}x_i\)</span> là ước lượng khả năng tối đa cần tìm</li>
</ol>
<hr />
<p>Nếu ta để ý thì  <span class="math">\(p = \frac{1}{n} ∑_{i=1}^{n}x_i\)</span> chính là công thức tính trung bình mẫu mà bạn (có thể) đã học trong xác suất thống kê. Điều này dẫn đến một triết lý &quot;Cái gì cũng có nguyên nhân của nó&quot;, ngụ ý rằng các công thức tưởng chừng khô khan và vô nghĩa vốn đã có nguồn gốc từ phương pháp Maximum likelihood kỳ diệu này.</p>
<p><em>Note: Có những tình huống bài toán giả định phân phối có 2 tham số (ví dụ phân phối chuẩn N(μ,σ)). Yêu cầu lúc này là ta phải ước lượng một trong 2 tham số đó với điều kiện là ta đã biết cái còn lại.</em></p>
<p>Với phương pháp MLE trên, người ta đã tìm ra công thức ước lượng 1 tham số cho tất cả các phân phối xác xuất khác nhau trong lịch sử. Ta có thể thấy điều đó thông qua bảng sau:</p>
<table x="">
<thead>
<tr>
<th>Phân phối</th>
<th>MLE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bernoulli(p)</td>
<td>\(p = \bar{x}\)</td>
</tr>
<tr>
<td>Normal (μ, <span class="math">\(σ^2\)</span>)</td>
<td><span class="math">\(μ = \bar{x}\)</span></td>
</tr>
<tr>
<td>Exp(λ)</td>
<td><span class="math">\(λ = \bar{x}^{-1}\)</span></td>
</tr>
<tr>
<td>Geometric(p)</td>
<td>\(p=\frac{1}{\bar{x}}\)</td>
</tr>
<tr>
<td>Binominal(p,n)</td>
<td><span class="math">\(p = \frac{\bar{x}}{n}\)</span></td>
</tr>
<tr>
<td>Poisson(λ)</td>
<td><span class="math">\(λ = \bar{x}\)</span></td>
</tr>
<tr>
<td>Uniform(p)</td>
<td><span class="math">\(θ = X_n\)</span></td>
</tr>
</tbody>
</table>
<h2 id="uoc-luong-nhieu-tham-so">Ước lượng nhiều tham số</h2>
<p>Lần này, thay vì chỉ ước lượng một thì chúng ta ước lượng nhiều tham số một chút. Các bước làm sẽ tương tự như ở trên, chỉ khác ở chỗ: <em>ta sẽ phải đạo hàm riêng nhiều lần</em></p>
<p><strong>Ví dụ</strong></p>
<p>Cho biến ngẫu nhiên <span class="math">\(X \in [x_1,x_2,...,x_n]\)</span> tuân theo phân phối chuẩn N(μ, <span class="math">\(σ^2\)</span>). Dùng phương pháp Ước lượng khả năng tối đa (MLE) để ước lượng μ và <span class="math">\(σ^2\)</span>.</p>
<p><em>Giải</em></p>
<ul>
<li><p>phân phối xác suất: <span class="math">\(\quad \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-μ)^2}{2\sigma^2}}\)</span></p>
</li>
<li><p>likelihood:</p>
</li>
</ul>
<p><span class="math">\(L(μ ,σ^2) = ∏_{i=1}^{n}\frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-μ)^2}{2\sigma^2}}\)</span></p>
<ul>
<li>log likelihood:</li>
</ul>
<p>Đặt <span class="math">\(l = log(L(μ ,σ^2))\)</span>, ta có:</p>
<p><span class="math">\(l = ∑_{i=1}^{n} log[\frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-μ)^2}{2\sigma^2}}]\)</span></p>
<p><span class="math">\(= ∑_{i=1}^{n} log(\frac{1}{\sigma \sqrt{2\pi}}) + ∑_{i = 1}^{n}log[ e^{-\frac{(x-μ)^2}{2\sigma^2}}]\)</span></p>
<p><span class="math">\(= - \frac{n}{2} log(\sigma^2) - - \frac{n}{2} log(2\pi) - \frac{1}{2\sigma^2} ∑_{i=1}^{n} (x_i - μ )^2\)</span></p>
<ul>
<li><p>Đạo hàm <span class="math">\(l\)</span> theo 2 biến μ  và <span class="math">\(σ^2\)</span></p>
<p><span class="math">\(\frac{\partial l}{\partial μ} = \frac{1}{\sigma^2} ∑_{i=1}^{n} (x_i - μ)\)</span></p>
<p><span class="math">\(\frac{\partial l}{\partial σ^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}∑_{i=1}^{n} (x_i - μ )^2\)</span></p>
</li>
<li><p>likelihood đạt tối đa</p>
</li>
</ul>
<div class="math">
\[
⟺ \begin{cases}
    \frac{\partial l}{\partial μ} = 0 \\
    \frac{\partial l}{\partial σ^2} = 0
\end{cases}
\]</div>
<div class="math">
\[
⟺ \begin{cases}
    μ = \bar{x} \\
    σ^2 = \frac{1}{n}∑_{i=1}^{n} x_i^2 - \bar{x}^2
\end{cases}
\]</div>
<h2>Ứng dụng</h2>
<p>MLE có rất nhiều ứng dụng (chủ yếu là dành cho dân thống kê và dân nghiên cứu) chẳng hạn như lý giải một hiện tượng nào đó có liên quan tới xác xuất.</p>
<p>Ví dụ:</p>
<ul>
<li>Tung đồng xu 10 lần. Khi đó, ta coi đồng xu là một biến ngẫu nhiên X chỉ nhận một trong 2 giá trị là &quot;sấp&quot; hoặc &quot;ngửa&quot;.
Việc tung 10 lần mang hàm ý là chúng ta lấy ra 10 mẫu dữ liệu. Và thoạt nhìn thì ta cũng biết phân phối dữ liệu là phân phối nhị thức (binominal).
Bộ tham số của phân phối lúc này là <span class="math">\(θ(n, p)\)</span>, trong đó n là số phép thử và p là xác xuất cho mỗi lần thử.</li>
</ul>
<p>MLE: <span class="math">\(L(n,p) = f(x | n, p) = P( X =  x_1,x_2,...x_n) = \binom{n}{x} p^x (1−p)^{n−x}\)</span></p>
<p>Một ví dụ khác, tưởng tượng bạn là một nhà nghiên cứu động vật và bạn đến Nam Cực để nghiên cứu chiều cao trung bình của những con chim cánh cụt, bạn sẽ làm gì để nghiên cứu ? Đơn giản nhất là đi bắt từng con (không phải kiểu săn bắn, bị còng đầu đấy :v) rồi đo chiều cao của chúng, sau đó tính trung bình.</p>
<p><img src="https://th.bing.com/th/id/OIP.dTNR-gcHgi1rEpKhBaqtDQHaE7?rs=1&amp;pid=ImgDetMain" alt="" /></p>
<p>Nhưng như tôi đã ví dụ ở đầu bài viết, bạn không thể đi khắp cái Nam Cực để làm điều đó (trừ khi bạn đủ kiên nhẫn và đủ tiền trang trải chi phí cho việc này). Bạn chỉ có thể loanh quanh đảo Snow Hill, một trong những nơi tập trung nhiều chim cánh cụt nhất ở Nam Cực. Khi đó, việc bạn làm là thu thập mẫu dữ liệu để nghiên cứu, và bạn giả định chiều cao của đám này tuân theo một theo quy luật nào đó. Tôi dám cá rằng bạn sẽ sử dụng MLE như một cách để ước tính các tham số của phân phối đó.</p>
<p>Thật đấy, bạn làm điều này mọi lúc mà có khi không biết. Khi bạn khảo sát trên một vài con đầu tiên, bạn sẽ thấy rằng kết quả thu được chưa đủ thuyết phục lắm, và bạn tiếp tục chọn ra những con tiếp theo. Cảm thấy chưa đủ thỏa mãn, bạn chọn một quần thể lớn hơn để tiếp tục đo đạc. Cứ hết lần này đến lần khác, bạn lấy ra ngẫu nhiên một bầy chim, thỉnh thoảng sẽ lòi ra một vài con bị trùng lặp với lần trước. Cho tới khi giá trị trung bình loanh quanh một kết quả nào đó không đổi, ví dụ 1m43 chẳng hạn. Lúc này, bạn đoán rằng con số 1m43 đó &quot;có vẻ&quot; cũng gần với chiều cao trung bình của chim cánh cụt của cả lục địa Nam Cực. Lý do thực sự khiến bạn làm điều này là vì bạn tin rằng giá trị trung bình của mẫu chính là giá trị tối đa của likelikhood mà bạn có thể ước tính. Tất nhiên, kết quả này chỉ củng cố niềm tin cho bạn thôi chứ nó không chứng minh rằng 1m43 là chiều cao trung bình của chim cánh cụt ở Nam Cực. Cho dù có thực hiện trên tất cả các khu vực khác của Nam Cực thì kết quả của bạn cũng chỉ là con số nhất thời vì biết đâu sau này người ta còn tìm ra kết quả mới hơn :))).</p>

</body>
</html>


